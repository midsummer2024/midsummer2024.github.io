<!DOCTYPE html>
<html>

<head>
    <title>Imitate4 Before Detect:</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');

    </script>

    

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="">Jiaqi Chen</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Xiaoye Zhu</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Tianyang Liu</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Ying Chen</a>,
                            </span>
							<span class="author-block">
                                <a href="">Xinhui Chen</a>,
                            </span>
                            <br>
                            <span class="author-block">
                                <a href="">Yiwen Yuan</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Chak Tou Leong</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Zuchao Li</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Tang Long</a>,
                            </span>
							<span class="author-block">
                                <a href="">Lei Zhang</a>,
                            </span>
                            <br>
							<span class="author-block">
                                <a href="">Chenyu Yan</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Guanghao Mei</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Jie Zhang</a>,
                            </span>
							<span class="author-block">
                                <a href="">Lefei Zhang</a>
                            </span>
                        </div>

						<!-- Remy
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> Google DeepMind</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><small>*Equal contribution, alphabetic order; work done at UC Berkeley</small></span>
						-->
							
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.11896" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DigiRL-agent/digirl" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://drive.google.com/drive/folders/14Iu6lAHePQ2qG0ghYkVG1RG6RUu7e2Hz?usp=sharing"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Data</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
	<!-- Abstract -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Large Language Models (LLMs) have revolutionized text generation, making detecting machine-generated text increasingly challenging. Although past methods have achieved good performance on detecting pure machine-generated text, those detectors have poor performance on distinguishing <b><i>machine-revised text</i></b> (rewriting, expansion, and polishing), which can have only minor changes from its original human prompt. As the content of text may originate from human prompts, detecting machine-revised text often involves identifying distinctive machine styles, e.g., worded favored by LLMs. However, existing methods struggle to detect machine-style phrasing hidden within the content contributed by humans. We propose the <b><i>“Imitate Before Detect” (ImBD)</i></b> approach, which first imitates the machine-style token distribution, and then compares the distribution of the text to be tested with the machine-style distribution to determine whether the text has been machinerevised. To this end, we introduce style preference optimization (SPO), which aligns a scoring LLM model to the preference of text styles generated by machines. The aligned scoring model is then used to calculate the style-conditional probability curvature (Style-CPC), quantifying the log probability difference between the original and conditionally sampled texts for effective detection. We conduct extensive comparisons across various scenarios, encompassing text revisions by six LLMs, four distinct text domains, and three machine revision types. Compared to existing state-of-the-art methods, our method yields a 13% increase in AUC for detecting text revised by open-source LLMs, and improves performance by 5% and 19% for detecting GPT-3.5 and GPT-4o revised text, respectively. Notably, our method surpasses the commercially trained GPT-Zero with just 1,000 samples and five minutes of SPO, demonstrating its efficiency and effectiveness. Code, data, and models are available.    
                        </p>
                    </div>
                </div>
            </div>			
        </div>
	<!-- End Abstract. -->
    </section>
	

    <section class="section" id="interaction-framework">
	<!-- Image 123 -->
        <div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h3> Why we need Imitate? </h3>
                        <div style="text-align:center;">
                            <img src="website/images/fig1.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 400px; width: 100%; height: auto;" />
                        </div>
						(a-c) Comparative examples of human-written, machine-generated, and machine-revised text. (d) Fast-DetectGPT shows a significant drop in detection accuracy when identifying machine-revised text compared to machinegenerated text. (e) Our method brings a noticeable improvement in detecting machine-revised text compared to Fast-DetectGPT. “Fast-Det.” denotes “Fast-DetectGPT”. <br>
                        						
						<div style="text-align:center;">
                            <img src="website/images/fig2.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 400px; width: 100%; height: auto;" />
                        </div>
						Imitating the stylistic preferences of LLMs. (a) Token distribution before and after machine-style imitation, demonstrating a deliberate fine-tuning of the scoring model to bias its token distribution towards a machine writing style (e.g., shifting preferences from common words like “explore” to machine-favored tokens such as “delve”). (b) The pipeline of Style Preference Optimization is applied to align the base scoring model with the style of machine-revised content using paired human-machine texts. This results in a machine-style scoring model, which generates token distributions p(x_n|x_(0:n?1)) for each position n, subsequently used for style-conditional probability curvature calculations. <br>
						
						<div style="text-align:center;">
                            <img src="website/images/fig3.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 400px; width: 100%; height: auto;" />
                        </div>
						Impact of Style-conditional probability curvatures (Style-CPC). (Left) Conditional probability curvatures (CPC) from Fast-DetectGPT (denoted as “Fast-Det.”) applied to purely machine-generated text; (Middle) Conditional probability curvatures applied to purely machine-revised text; (Right) Style-conditional probability curvatures from ours applied to machine-revised text. The greater the separation between human-written texts (red) and machine-revised texts (blue), the more effective the detection. <br>
                    </div>
                </div>
                <!--/ Visual Effects. -->
			</div>
            </div>
		</div>
	<!-- End Image 123 -->
    </section>


	<section class="hero teaser">
	<!-- Data Table -->
        <div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
            <div class="hero-body">

                <h3> Performance </h3>
                Detection of GPT-3.5 and GPT-4o polished text. Typically, the Neo-2.7B (Black et al. 2021) is used as the source for the scoring model. NPR and DetectGPT, on the other hand, utilize T5-3B (Chen et al. 2019) for generating perturbations, whereas Fast-DetectGPT employs GPT-J (Wang and Komatsuzaki 2021) as a surrogate model to generate samples. The ? symbol denotes methods that require multiple model invocations, leading to a substantial increase in computational load. Metric: AUROC.
            </div>
			</div>
			</div>
        </div>
    <!-- End Data Table -->
	</section>

	
	<!-- Web tables -->
	<section class="section">
      <div class="container">

          <div class="columns is-centered">
              <div class="columns is-centered has-text-centered">

                  <!--<h2 class="title is-3" id="leaderboard">Table Caption xxx</h2>-->
                    <div class="content has-text-justified">
                      <table>

                        <thead>
                          <tr></tr>
                          <tr>
                            <th rowspan="2" style="vertical-align: middle;">Method</th>
                            <th colspan="1" style="text-align: center; font-weight: bold;">Time cost</th>
							<th colspan="3" style="text-align: center; font-weight: bold;">GPT-3.5</th>
                            <th rowspan="2" style="vertical-align: middle;">Avg.</th>
							<th colspan="3" style="text-align: center; font-weight: bold;">GPT-4o</th>
                            <th rowspan="2" style="vertical-align: middle;">Avg.</th>
                          </tr>

                          <tr>
                            <th>(s/1k words)</th>
                            <th>XSum</th>
                            <th>Writing</th>
                            <th>PubMed</th>
							<th>XSum</th>
                            <th>Writing</th>
                            <th>PubMed</th>
                          </tr>
                        </thead>

                        <tbody id="tabResults">

                          <tr>
                            <td>RoBERTa-base</td>
                            <td><b>0.07</b></td>
                            <td>0.5806</td>
                            <td>0.7225</td>
                            <td>0.4370</td>
                            <td>0.5800</td>
							<td>0.4921</td>
                            <td>0.4774</td>
                            <td>0.2496</td>
                            <td>0.4064</td>
                          </tr>
						  
						  <tr>
                            <td>RoBERTa-large</td>
                            <td>0.11</td>
                            <td>0.6391</td>
                            <td>0.7236</td>
                            <td>0.4848</td>
                            <td>0.6158</td>
							<td>0.4782</td>
                            <td>0.4708</td>
                            <td>0.3089</td>
                            <td>0.4193</td>
                          </tr>
						  
						  <tr>
                            <td>Likehood</td>
                            <td>0.38</td>
                            <td>0.4982</td>
                            <td>0.8788</td>
                            <td>0.5528</td>
                            <td>0.6433</td>
							<td>0.4396</td>
                            <td>0.8077</td>
                            <td>0.4596</td>
                            <td>0.5690</td>
                          </tr>
						  
						  <tr>
                            <td>Entropy</td>
                            <td>0.35</td>
                            <td>0.6742</td>
                            <td>0.3021</td>
                            <td>0.5662</td>
                            <td>0.5142</td>
							<td>0.6122</td>
                            <td>0.2802</td>
                            <td>0.5899</td>
                            <td>0.4941</td>
                          </tr>
						  
						  <tr>
                            <td>LogRank</td>
                            <td>0.36</td>
                            <td>0.4711</td>
                            <td>0.8496</td>
                            <td>0.5597</td>
                            <td>0.6268</td>
							<td>0.4002</td>
                            <td>0.7694</td>
                            <td>0.4472</td>
                            <td>0.5389</td>
                          </tr>
						  
						  <tr>
                            <td>LRR</td>
                            <td>0.41</td>
                            <td>0.4016</td>
                            <td>0.7203</td>
                            <td>0.5629</td>
                            <td>0.5616</td>
							<td>0.3095</td>
                            <td>0.6214</td>
                            <td>0.4710</td>
                            <td>0.4673</td>
                          </tr>
						  
						  <tr>
                            <td>DNA-GPT</td>
                            <td>35.92</td>
                            <td>0.5338</td>
                            <td>0.8439</td>
                            <td>0.3333</td>
                            <td>0.5703</td>
							<td>0.4974</td>
                            <td>0.7478</td>
                            <td>0.3151</td>
                            <td>0.5201</td>
                          </tr>
						  
						  <tr>
                            <td>NPR</td>
                            <td>111.99</td>
                            <td>0.5659</td>
                            <td>0.8786</td>
                            <td>0.4246</td>
                            <td>0.6230</td>
							<td>0.5065</td>
                            <td>0.8444</td>
                            <td>0.3740</td>
                            <td>0.5750</td>
                          </tr>
						  
						  <tr>
                            <td>DetectGPT</td>
                            <td>111.33</td>
                            <td>0.6343</td>
                            <td>0.8793</td>
                            <td>0.5608</td>
                            <td>0.6915</td>
							<td>0.6217</td>
                            <td>0.8771</td>
                            <td>0.5612</td>
                            <td>0.6867</td>
                          </tr>
						  
						  <tr>
                            <td>Fast-DetectGPT</td>
                            <td>0.72</td>
                            <td>0.7312</td>
                            <td>0.9304</td>
                            <td>0.7182</td>
                            <td>0.7933</td>
							<td>0.6293</td>
                            <td>0.8324</td>
                            <td>0.6175</td>
                            <td>0.6931</td>
                          </tr>
						  
						  <tr>
                            <td>ImBD (Ours)</td>
                            <td>0.72</td>
                            <td><b>0.9849</b></td>
                            <td><b>0.9871</b></td>
                            <td><b>0.8626</b></td>
                            <td><b>0.9449</b></td>
							<td><b>0.9486</b></td>
                            <td><b>0.9468</b></td>
                            <td><b>0.7743</b></td>
                            <td><b>0.8899</b></td>
                          </tr>

                        </tbody>
                      </table>
                    </div>

              </div>
          </div>

      </div>
	</section>

	<section class="section">
      <div class="container">

          <div class="columns is-centered">
              <!--<div class="columns is-centered has-text-centered">-->
              <div class="column is-four-fifths">
                  <h2 class="title is-3" id="leaderboard"></h2>
                    <div class="content has-text-justified">
                      <table>

                        <thead>
                          <tr></tr>
                          <tr>
                            <th>Method</th>
                            <th>XSum</th>
                            <th>Writing</th>
                            <th>PubMed</th>
							<th>Avg.</th>
                          </tr>
                        </thead>

                        <tbody id="tabResults">

                          <tr>
                            <td>GPTZero</td>
                            <td>0.9542</td>
                            <td>0.9711</td>
                            <td><b>0.8800</b></td>
							<td>0.9351</td>
                          </tr>
						  
						  <tr>
                            <td>ImBD (Ours)</td>
                            <td><b>0.9849</b></td>
                            <td><b>0.9871</b></td>
                            <td>0.8626</td>
                            <td><b>0.9449</b></td>
                          </tr>

                        </tbody>
                      </table>
					  <p align="center">Table 2: <b>Compared with GPTZero on detecting <i>GPT-3.5</i> polished text.</b> Metric: AUROC.</p>
                    </div>
              <!--</div>-->
              </div>
          </div>

      </div>
	</section>
	
	<section class="section">
      <div class="container">

          <div class="columns is-centered">
              <!--<div class="columns is-centered has-text-centered">-->
              <div class="column is-four-fifths">
                  <h2 class="title is-3" id="leaderboard"></h2>
				  <br>
                    <div class="content has-text-justified">
                      <table>

                        <thead>
                          <tr></tr>
                          <tr>
                            <th rowspan="2" style="vertical-align: middle;">Method</th>
                            <th colspan="4" style="text-align: center; font-weight: bold;">Tasks</th>
                            <th rowspan="2" style="vertical-align: middle;">Avg.</th>
                          </tr>

                          <tr>
                            <th>Rewrite</th>
                            <th>Expand</th>
                            <th>Polish</th>
                            <th>Generate</th>
                          </tr>
                        </thead>

                        <tbody id="tabResults">

                          <tr>
                            <td>Likehood</td>
                            <td>0.4073</td>
                            <td>0.4564</td>
                            <td>0.6039</td>
                            <td>0.8939</td>
                            <td>0.5904</td>
                          </tr>
						  
						  <tr>
                            <td>Entropy</td>
                            <td>0.5840</td>
                            <td>0.6629</td>
                            <td>0.5431</td>
                            <td>0.4129</td>
                            <td>0.5507</td>
                          </tr>
						  
						  <tr>
                            <td>LogRank</td>
                            <td>0.3868</td>
                            <td>0.4273</td>
                            <td>0.5864</td>
                            <td>0.8925</td>
                            <td>0.5732</td>
                          </tr>
						  
						  <tr>
                            <td>LRR</td>
                            <td>0.3488</td>
                            <td>0.3581</td>
                            <td>0.5183</td>
                            <td>0.8541</td>
                            <td>0.5198</td>
                          </tr>
						  
						  <tr>
                            <td>DNA-GPT</td>
                            <td>0.4101</td>
                            <td>0.4901</td>
                            <td>0.5847</td>
                            <td>0.8931</td>
                            <td>0.5945</td>
                          </tr>
						  
						  <tr>
                            <td>NPR</td>
                            <td>0.3606</td>
                            <td>0.5139</td>
                            <td>0.5673</td>
                            <td>0.8541</td>
                            <td>0.5740</td>
                          </tr>
						  
						  <tr>
                            <td>DetectGPT</td>
                            <td>0.4060</td>
                            <td>0.6000</td>
                            <td>0.6615</td>
                            <td>0.8985</td>
                            <td>0.6415</td>
                          </tr>
						  
						  <tr>
                            <td>Fast-DetectGPT</td>
                            <td>0.4499</td>
                            <td>0.7159</td>
                            <td>0.7989</td>
                            <td>0.9706</td>
                            <td>0.7338</td>
                          </tr>
						  
						  <tr>
                            <td>ImBD (Ours)</td>
                            <td><b>0.8739</b></td>
                            <td><b>0.9758</b></td>
                            <td><b>0.9707</b></td>
                            <td><b>0.9996</b></td>
                            <td><b>0.9550</b></td>
                          </tr>

                        </tbody>
                      </table>
					  <p align="center">Table 3: <b>Performance on diverse tasks.</b> We evaluated the detection performance, measured by average AUROC, of text revised by leading LLMs (Qwen2-7B, Llama-3-8B, Mixtral-7B, Deepseek-7B, GPT-3.5, and GPT-4o) on the XSum dataset.</p>
                    </div>
			  <!--</div>-->
              </div>
          </div>

      </div>
	</section>
	<!-- End Web tables -->
	

     <section class="section" id="interaction-framework">
	<!-- Image 45 -->
        <div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h3> Extra Experiment on Text Length </h3>
                        Evaluations of detection accuracy for XSum polished texts trimmed to the specified word count. <br>
                        <div style="text-align:center;">
                            <img src="website/images/fig4.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 450px; width: 100%; height: auto;" />
                            <br>
                        </div>
                        
						<h3> More Visualization </h3>
                        ROC curves in log scale evaluated on polish task of XSum dataset, where the dash lines denote the random classifier. “Fast-Det.” denotes “Fast-DetectGPT”. <br>
						<div style="text-align:center;">
                            <img src="website/images/FPR.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 500px; width: 100%; height: auto;" />
                            <br>
                        </div>
						
						Additional performance comparison on detecting machine-polished text. Target LLM: GPT-3.5.
						<div style="text-align:center;">
                            <img src="website/images/AUROCBars1.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 600px; width: 100%; height: auto;" />
                            <br>
                        </div>
                    </div>
                </div>
                <!--/ Visual Effects. -->
			</div>
            </div>
		</div>
	<!-- End Image 45 -->
    </section> 


    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
